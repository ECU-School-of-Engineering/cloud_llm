# -*- coding: utf-8 -*-
"""Hume_CLM_LLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14hOdY_HFoShrTZkviCMIuiVKV9IN9JQB

Dependencies
"""


"""Load Model"""

# ðŸ§  Load the LLM - LLama
from huggingface_hub import hf_hub_download
from llama_cpp import Llama


#Stheno- In terminal huggingface-cli download bartowski/L3-8B-Stheno-v3.2-GGUF --include "L3-8B-Stheno-v3.2-Q4_K_M.gguf" --local-dir ./models/stheno
model_path=".models/stheno/L3-8B-Stheno-v3.2-Q4_K_M.gguf"
#  Step 2: Version 2 - Test
llm = Llama(
    model_path=model_path,
    n_gpu_layers=-1,       # âœ… Usa todas las capas en GPU si caben
    n_ctx=4096,
    n_threads=8,           # âœ… o mÃ¡s, si tienes mÃ¡s vCPUs
    n_batch=512,           # âœ… Aumenta throughput (puedes probar hasta 1024)
)

print("âœ… Model loaded successfully.")

"""LLM prompt handler"""

# ðŸ§¾ Generate assistant reply based on OpenAI-style chat messages
def generate_response(messages: list[dict]) -> str:
    import re
    def clean_labels(text: str) -> str:
        # Remove lines starting with up to 2 words followed by a colon
          return re.sub(r"(?m)^\s*(\w+\s?){1,2}:\s*", "", text)

    # ðŸ§  Define the system prompt (character + tone)
    system_prompt = """
    You are playing the role of a patient in a busy hospital in Melbourne, Australia.

    Your character:
    - You're "Barry", the patient, a rough-around-the-edges Australian man in his 40s.
    - Youâ€™re annoyed because you've been waiting for hours.
    - You have a sharp pain in your left shoulder and want painkillers.
    - Youâ€™re violent, blunt, rude, and impatient and use curse words.
    - Use casual Australian slang and expressions where appropriate.
    - Donâ€™t sound like a robot. Be natural, frustrated, and honest.
    - You refuse to leave until someone gives you something for the pain.
    Important:
    - Do not use symbols, just plain text
    - Keep it simple and conversational, no endless responses
    - Stay in character as if you're talking to a nurse.
    """

    # ðŸ§¾ Format conversation history
    prompt = system_prompt + "\n"
    for msg in messages:
        role = msg.get("role")
        content = msg.get("content")
        if role == "user":
            prompt += f"Nurse: {content}\n"
        elif role == "assistant":
            prompt += f"Patient: {content}\n"
    prompt += ""

    print("ðŸ“ Prompt sent to LLM:\n", prompt)

    # ðŸ¤– Generate response
    output = llm(prompt, max_tokens=150, stop=["Nurse:", "\nNurse:"])
    result = output["choices"][0]["text"].strip()

    print("ðŸ¤– LLM raw output:\n", clean_labels(result))

    return clean_labels(result)

"""FastAPI + Ngrok"""

# ðŸš€ FastAPI backend with OpenAI-compatible streaming response
import nest_asyncio
import asyncio
import json
import uuid
import time
from typing import AsyncIterable, Optional
from fastapi import FastAPI, Request
from fastapi.responses import StreamingResponse
from pyngrok import ngrok

# Allow nested event loops for Colab
nest_asyncio.apply()

# Initialize FastAPI app
app = FastAPI()

# Authenticate Ngrok
NGROK_TOKEN = "2ytboIPV8DGyHpEJjFUB0JkW1Dl_4g96eqRnWZvr4U1kKrvrH"
ngrok.set_auth_token(NGROK_TOKEN)
public_url = ngrok.connect(8000)
print(f"âœ… Public API URL: {public_url}/v1/chat/completions")

# SSE-compatible streaming endpoint
@app.post("/v1/chat/completions", response_class=StreamingResponse)
async def chat_completions(request: Request):
    print("ðŸš¨ Incoming request to /v1/chat/completions")

    try:
        body = await request.json()
        print("ðŸ” Raw body:", body)
    except Exception as e:
        print("âŒ Error parsing JSON:", e)
        return {"error": "Invalid JSON format"}

    messages = body.get("messages", [])

    if messages:
        last = messages[-1]
        print(f"ðŸ“¥ Last message received ({last.get('role')}): {last.get('content')}")
    else:
        print("âš ï¸ No message content received.")

    return StreamingResponse(
        get_response(messages),
        media_type="text/event-stream"
    )

# Generate streaming chunks (OpenAI-style)
async def get_response(messages: list[dict], custom_session_id: Optional[str] = None) -> AsyncIterable[str]:
    base_id = f"chatcmpl_{uuid.uuid4().hex[:8]}"
    created = int(time.time())

    full_response = generate_response(messages)
    print("ðŸ“¤ Streaming response to Hume:")
    print(full_response)

    for word in full_response.split():
        chunk = {
            "id": base_id,
            "object": "chat.completion.chunk",
            "created": created,
            "model": "dolphin-2.8-mistral-7b-v02.GGUF",
            "choices": [
                {
                    "delta": {"content": word + " "},
                    "index": 0,
                    "finish_reason": None
                }
            ]
        }
        yield f"data: {json.dumps(chunk)}\n\n"
        await asyncio.sleep(0.03)

    final_chunk = {
        "id": base_id,
        "object": "chat.completion.chunk",
        "created": created,
        "model": "dolphin-2.8-mistral-7b-v02.GGUF",
        "choices": [
            {
                "delta": {},
                "index": 0,
                "finish_reason": "stop"
            }
        ]
    }
    yield f"data: {json.dumps(final_chunk)}\n\n"
    yield "data: [DONE]\n\n"

# ðŸš€ Launch the server
import uvicorn

uvicorn.run(app, host="0.0.0.0", port=8000, log_level="debug", access_log=True)